- Add to requirements.txt the folowing packages (once confirmed that file generation works): python-docx fpdf markdown
- Look in the code for other imports that may require package installation in the venv
- Specify package versions in requirements.txt
- Check whether I can save token length by not returning COT of the model
- Why the interface sometimes do not process documents, depending on my desktop activity?
- Allow usage of local model. For example with OLLAMA. But make sure that the default model is lightweight so it can fit in no-GPU machines with, say, 16GB RAM.
- Create file with generated text
- Remove emojis (at least in Gradio interface)
- Admit all possible formats for generated response (the raw LLM response)
- Explore inference methods other than InferenceClient.text_generation (nor working) or .chat_completion
- Comment on the usage of FAISS
- Add .env_example
- Make Gradio interface look nicer
