- Add to requirements.txt the foolowing packages (once confirmed that file generation works): python-docx fpdf markdown
- Check whether I can save token length by not returning COT of the model
- Why the interface sometimes do not process documents, depending on my desktop activity?
- Allow usage of local model
- Create file with generated text
- Remove emojis (at least in Gradio interface)
- Admit all possible formats for generated response (the raw LLM response)
- Explore inference methods other than InferenceClient.text_generation (nor working) or .chat_completion
- Comment on the usage of FAISS
- Add .env_example
- Make Gradio interface look nicer
